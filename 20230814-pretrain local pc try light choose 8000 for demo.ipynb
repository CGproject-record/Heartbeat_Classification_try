{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a568207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20230803-pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499abcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\pek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From c:\\users\\pek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\pek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\users\\pek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 64, 16)       192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 16)       64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 16)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 31, 16)       0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 27, 32)       2592        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 27, 32)       128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 27, 32)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 13, 32)       0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 11, 64)       6208        max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 11, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 11, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5, 64)        0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 320)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 324)          0           flatten_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           20800       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            260         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,500\n",
      "Trainable params: 30,276\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n",
      "training:\n",
      "8000/8000 [==============================] - 0s 23us/step\n",
      "[[7417    1    0    0]\n",
      " [   2    5    0    0]\n",
      " [   2    1  568    0]\n",
      " [   4    0    0    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.999     1.000     0.999      7418\n",
      "           1      0.714     0.714     0.714         7\n",
      "           2      1.000     0.995     0.997       571\n",
      "           3      0.000     0.000     0.000         4\n",
      "\n",
      "    accuracy                          0.999      8000\n",
      "   macro avg      0.678     0.677     0.678      8000\n",
      "weighted avg      0.998     0.999     0.998      8000\n",
      "\n",
      "testing:\n",
      "8000/8000 [==============================] - 0s 12us/step\n",
      "[[7825    0   79   18]\n",
      " [  19   16    0    0]\n",
      " [   0    1   42    0]\n",
      " [   0    0    0    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.998     0.988     0.993      7922\n",
      "           1      0.941     0.457     0.615        35\n",
      "           2      0.347     0.977     0.512        43\n",
      "           3      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.985      8000\n",
      "   macro avg      0.571     0.605     0.530      8000\n",
      "weighted avg      0.994     0.985     0.988      8000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\pek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    y_pred = K.one_hot(K.argmax(y_pred, axis=-1), num_classes=4)\n",
    "\n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    tn = K.sum((1 - y_true) * (1 - y_pred), axis=0)\n",
    "    fp = K.sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = K.sum(y_true * (1 - y_pred), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    return K.mean(2 * precision * recall / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "def categorical_focal_loss(gamma=2):\n",
    "    \"\"\"\n",
    "        Categorical form of focal loss.\n",
    "            FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "        References:\n",
    "            https://arxiv.org/pdf/1708.02002.pdf\n",
    "        Usage:\n",
    "            model.compile(loss=categorical_focal_loss(gamma=2), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model.fit(class_weight={0:alpha0, 1:alpha1, ...}, ...)\n",
    "        Notes:\n",
    "           1. The alpha variable is the class_weight of keras.fit, so in implementation of the focal loss function\n",
    "           we needn't define this variable.\n",
    "           2. (important!!!) The output of the loss is the loss value of each training sample, not the total or average\n",
    "            loss of each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n",
    "        y_true = K.cast(y_true, y_pred.dtype)\n",
    "\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "        return K.sum(-y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred), axis=-1)\n",
    "\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "def load_data(filename=\"./mitdb_light_8000.pkl\"):\n",
    "    import pickle\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = pickle.load(f)\n",
    "\n",
    "    return (x1_train, x2_train, y_train), (x1_test, x2_test, y_test)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = load_data()\n",
    "\n",
    "    x1_train = np.expand_dims(x1_train, axis=-1)\n",
    "    x1_test = np.expand_dims(x1_test, axis=-1)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    x2_train = scaler.fit_transform(x2_train)\n",
    "    x2_test = scaler.transform(x2_test)\n",
    "\n",
    "    model = load_model(os.path.join(\"./models\", \"model_focalloss.h5\"),\n",
    "                       custom_objects={\"focal_loss\": categorical_focal_loss(gamma=2),\n",
    "                                       \"f1\": f1})\n",
    "    model.summary()\n",
    "\n",
    "    print(\"training:\")\n",
    "    y_true, y_pred = y_train, np.argmax(model.predict([x1_train, x2_train], batch_size=1024, verbose=1), axis=-1)\n",
    "\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "    print(\"testing:\")\n",
    "    y_true, y_pred = y_test, np.argmax(model.predict([x1_test, x2_test], batch_size=1024, verbose=1), axis=-1)\n",
    "\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91424e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: [7.418e+03 7.000e+00 5.710e+02 4.000e+00]\n",
      "test labels: [7922.   35.   43.    0.]\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 200, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 64, 16)       192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 16)       64          conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 16)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 31, 16)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 27, 32)       2592        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 27, 32)       128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 27, 32)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 13, 32)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 11, 64)       6208        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 11, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 11, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 5, 64)        0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 320)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 324)          0           flatten_2[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           20800       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            260         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,500\n",
      "Trainable params: 30,276\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.9083 - acc: 0.8005 - f1: 0.2787 - val_loss: 0.4860 - val_acc: 0.8574 - val_f1: 0.2488\n",
      "WARNING:tensorflow:From c:\\users\\pek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.3799 - acc: 0.9837 - f1: 0.4697 - val_loss: 0.4363 - val_acc: 0.9779 - val_f1: 0.3250\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.3643 - acc: 0.9936 - f1: 0.4898 - val_loss: 0.4280 - val_acc: 0.9685 - val_f1: 0.3057\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3552 - acc: 0.9956 - f1: 0.4934 - val_loss: 0.4106 - val_acc: 0.9864 - val_f1: 0.3655\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 1s 31ms/step - loss: 0.3474 - acc: 0.9960 - f1: 0.4936 - val_loss: 0.3928 - val_acc: 0.9875 - val_f1: 0.3699\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.3401 - acc: 0.9964 - f1: 0.4944 - val_loss: 0.3665 - val_acc: 0.9893 - val_f1: 0.3807\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3328 - acc: 0.9967 - f1: 0.4957 - val_loss: 0.3512 - val_acc: 0.9905 - val_f1: 0.3920\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3266 - acc: 0.9974 - f1: 0.4966 - val_loss: 0.3486 - val_acc: 0.9908 - val_f1: 0.4007\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3189 - acc: 0.9976 - f1: 0.4972 - val_loss: 0.3253 - val_acc: 0.9914 - val_f1: 0.4084\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3152 - acc: 0.9977 - f1: 0.4973 - val_loss: 0.3116 - val_acc: 0.9912 - val_f1: 0.4164\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3140 - acc: 0.9979 - f1: 0.4976 - val_loss: 0.3502 - val_acc: 0.9911 - val_f1: 0.3871\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3131 - acc: 0.9976 - f1: 0.4972 - val_loss: 0.3067 - val_acc: 0.9911 - val_f1: 0.4185\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3125 - acc: 0.9979 - f1: 0.4972 - val_loss: 0.3383 - val_acc: 0.9911 - val_f1: 0.4215\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3116 - acc: 0.9979 - f1: 0.4972 - val_loss: 0.3049 - val_acc: 0.9910 - val_f1: 0.4154\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3108 - acc: 0.9979 - f1: 0.4978 - val_loss: 0.3254 - val_acc: 0.9911 - val_f1: 0.4200\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3099 - acc: 0.9979 - f1: 0.4976 - val_loss: 0.3113 - val_acc: 0.9910 - val_f1: 0.3945\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3093 - acc: 0.9979 - f1: 0.4969 - val_loss: 0.3159 - val_acc: 0.9910 - val_f1: 0.4109\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3082 - acc: 0.9980 - f1: 0.4978 - val_loss: 0.3302 - val_acc: 0.9911 - val_f1: 0.4189\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3076 - acc: 0.9980 - f1: 0.4977 - val_loss: 0.3463 - val_acc: 0.9911 - val_f1: 0.4271\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3073 - acc: 0.9981 - f1: 0.4977 - val_loss: 0.3125 - val_acc: 0.9910 - val_f1: 0.4187\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3070 - acc: 0.9981 - f1: 0.4980 - val_loss: 0.3145 - val_acc: 0.9910 - val_f1: 0.4199\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3069 - acc: 0.9980 - f1: 0.4977 - val_loss: 0.3167 - val_acc: 0.9909 - val_f1: 0.4014\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3069 - acc: 0.9980 - f1: 0.4978 - val_loss: 0.3312 - val_acc: 0.9906 - val_f1: 0.4049\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3068 - acc: 0.9980 - f1: 0.4975 - val_loss: 0.3121 - val_acc: 0.9908 - val_f1: 0.4136\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3065 - acc: 0.9980 - f1: 0.4977 - val_loss: 0.3016 - val_acc: 0.9908 - val_f1: 0.3954\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3063 - acc: 0.9980 - f1: 0.4976 - val_loss: 0.3369 - val_acc: 0.9905 - val_f1: 0.3945\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3064 - acc: 0.9979 - f1: 0.4975 - val_loss: 0.3204 - val_acc: 0.9904 - val_f1: 0.3999\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3061 - acc: 0.9980 - f1: 0.4977 - val_loss: 0.3109 - val_acc: 0.9902 - val_f1: 0.3989\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3060 - acc: 0.9980 - f1: 0.4978 - val_loss: 0.3028 - val_acc: 0.9901 - val_f1: 0.3911\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3063 - acc: 0.9980 - f1: 0.4975 - val_loss: 0.3306 - val_acc: 0.9901 - val_f1: 0.3839\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3066 - acc: 0.9980 - f1: 0.4976 - val_loss: 0.2999 - val_acc: 0.9898 - val_f1: 0.3891\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3059 - acc: 0.9980 - f1: 0.4976 - val_loss: 0.3194 - val_acc: 0.9896 - val_f1: 0.4069\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3063 - acc: 0.9980 - f1: 0.4975 - val_loss: 0.3176 - val_acc: 0.9896 - val_f1: 0.3824\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3060 - acc: 0.9980 - f1: 0.4976 - val_loss: 0.3095 - val_acc: 0.9896 - val_f1: 0.3687\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3059 - acc: 0.9980 - f1: 0.4978 - val_loss: 0.3180 - val_acc: 0.9896 - val_f1: 0.3843\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3058 - acc: 0.9980 - f1: 0.4977 - val_loss: 0.3042 - val_acc: 0.9896 - val_f1: 0.3815\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.3058 - acc: 0.9980 - f1: 0.4976 - val_loss: 0.3082 - val_acc: 0.9896 - val_f1: 0.3811\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3059 - acc: 0.9981 - f1: 0.4978 - val_loss: 0.3374 - val_acc: 0.9896 - val_f1: 0.3840\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3059 - acc: 0.9980 - f1: 0.4978 - val_loss: 0.3416 - val_acc: 0.9896 - val_f1: 0.3884\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3066 - acc: 0.9981 - f1: 0.4976 - val_loss: 0.3290 - val_acc: 0.9896 - val_f1: 0.3929\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3070 - acc: 0.9980 - f1: 0.4978 - val_loss: 0.3218 - val_acc: 0.9898 - val_f1: 0.3844\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3058 - acc: 0.9980 - f1: 0.4975 - val_loss: 0.3150 - val_acc: 0.9898 - val_f1: 0.3762\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3063 - acc: 0.9980 - f1: 0.4975 - val_loss: 0.3281 - val_acc: 0.9898 - val_f1: 0.3831\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3058 - acc: 0.9980 - f1: 0.4977 - val_loss: 0.3222 - val_acc: 0.9898 - val_f1: 0.3887\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.3058 - acc: 0.9980 - f1: 0.4976 - val_loss: 0.3021 - val_acc: 0.9898 - val_f1: 0.3925\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3059 - acc: 0.9980 - f1: 0.4975 - val_loss: 0.3189 - val_acc: 0.9898 - val_f1: 0.3902\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3058 - acc: 0.9980 - f1: 0.4977 - val_loss: 0.3222 - val_acc: 0.9896 - val_f1: 0.3834\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.3065 - acc: 0.9980 - f1: 0.4977 - val_loss: 0.3234 - val_acc: 0.9896 - val_f1: 0.3842\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3059 - acc: 0.9980 - f1: 0.4976 - val_loss: 0.3316 - val_acc: 0.9896 - val_f1: 0.3860\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3057 - acc: 0.9980 - f1: 0.4977 - val_loss: 0.3479 - val_acc: 0.9896 - val_f1: 0.3924\n",
      "8000/8000 [==============================] - 0s 35us/step\n",
      "[[7875    0   47]\n",
      " [  23    0   12]\n",
      " [   1    0   42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.997     0.994     0.996      7922\n",
      "           1      0.000     0.000     0.000        35\n",
      "           2      0.416     0.977     0.583        43\n",
      "\n",
      "    accuracy                          0.990      8000\n",
      "   macro avg      0.471     0.657     0.526      8000\n",
      "weighted avg      0.989     0.990     0.989      8000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Activation, BatchNormalization, Concatenate, Conv1D, Dense, Flatten, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "np.random.seed(0)\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    y_pred = K.one_hot(K.argmax(y_pred, axis=-1), num_classes=4)\n",
    "\n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    tn = K.sum((1 - y_true) * (1 - y_pred), axis=0)\n",
    "    fp = K.sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = K.sum(y_true * (1 - y_pred), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    return K.mean(2 * precision * recall / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "def categorical_focal_loss(gamma=2):\n",
    "    \"\"\"\n",
    "        Categorical form of focal loss.\n",
    "            FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "        References:\n",
    "            https://arxiv.org/pdf/1708.02002.pdf\n",
    "        Usage:\n",
    "            model.compile(loss=categorical_focal_loss(gamma=2), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model.fit(class_weight={0:alpha0, 1:alpha1, ...}, ...)\n",
    "        Notes:\n",
    "           1. The alpha variable is the class_weight of keras.fit, so in implementation of the focal loss function\n",
    "           we needn't define this variable.\n",
    "           2. (important!!!) The output of the loss is the loss value of each training sample, not the total or average\n",
    "            loss of each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n",
    "        y_true = K.cast(y_true, y_pred.dtype)\n",
    "\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "        return K.sum(-y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred), axis=-1)\n",
    "\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "def create_model(l=0.0):\n",
    "    inputs1 = Input(shape=(200, 1))\n",
    "    x1 = inputs1\n",
    "\n",
    "    x1 = Conv1D(16, kernel_size=11, strides=3, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l),\n",
    "                bias_regularizer=l2(l))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(\"relu\")(x1)\n",
    "\n",
    "    x1 = MaxPooling1D(3, strides=2)(x1)\n",
    "\n",
    "    x1 = Conv1D(32, kernel_size=5, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l),\n",
    "                bias_regularizer=l2(l))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(\"relu\")(x1)\n",
    "\n",
    "    x1 = MaxPooling1D(3, strides=2)(x1)\n",
    "\n",
    "    x1 = Conv1D(64, kernel_size=3, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l),\n",
    "                bias_regularizer=l2(l))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(\"relu\")(x1)\n",
    "\n",
    "    x1 = MaxPooling1D(3, strides=2)(x1)\n",
    "\n",
    "    x1 = Flatten()(x1)\n",
    "\n",
    "    inputs2 = Input(shape=(4,))\n",
    "    x2 = inputs2\n",
    "\n",
    "    x = Concatenate()([x1, x2])\n",
    "\n",
    "    x = Dense(64, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l), bias_regularizer=l2(l),\n",
    "              activation=\"relu\")(x)\n",
    "\n",
    "    outputs = Dense(4, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=(inputs1, inputs2), outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class MyGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x1, x2, y, batch_size):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(self.x1))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x1) // self.batch_size + 1\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        x1_batch = self.x1[self.indices[item * self.batch_size:(item + 1) * self.batch_size]]\n",
    "        x2_batch = self.x2[self.indices[item * self.batch_size:(item + 1) * self.batch_size]]\n",
    "        y_batch = self.y[self.indices[item * self.batch_size:(item + 1) * self.batch_size]]\n",
    "        return [x1_batch, x2_batch], y_batch\n",
    "\n",
    "\n",
    "def load_data(filename=\"mitdb_light_8000.pkl\"):\n",
    "    import pickle\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = pickle.load(f)\n",
    "\n",
    "    return (x1_train, x2_train, y_train), (x1_test, x2_test, y_test)\n",
    "\n",
    "\n",
    "def main():\n",
    "    epochs = 50\n",
    "    batch_size = 512\n",
    "\n",
    "    # loading data\n",
    "    (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = load_data()\n",
    "\n",
    "    x1_train = np.expand_dims(x1_train, axis=-1)\n",
    "    x1_test = np.expand_dims(x1_test, axis=-1)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    x2_train = scaler.fit_transform(x2_train)\n",
    "    x2_test = scaler.transform(x2_test)\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=4)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=4)\n",
    "\n",
    "    print(\"train labels:\", np.sum(y_train, axis=0))\n",
    "    print(\"test labels:\", np.sum(y_test, axis=0))\n",
    "\n",
    "    train_generator = MyGenerator(x1_train, x2_train, y_train, batch_size)\n",
    "    test_generator = MyGenerator(x1_test, x2_test, y_test, batch_size)\n",
    "\n",
    "    model = create_model(l=1e-3)\n",
    "    model.summary()\n",
    "\n",
    "    # callbacks\n",
    "    log_dir = os.path.join(\"\", datetime.now().strftime(\"%H-%M-%S\"))\n",
    "    tb_cb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    def schedule(epoch, lr):\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            lr *= 0.1\n",
    "        return lr\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(schedule=schedule, verbose=0)\n",
    "\n",
    "    # training\n",
    "    model.compile(loss=categorical_focal_loss(gamma=2), optimizer=\"adam\", metrics=[\"acc\", f1])\n",
    "    model.fit_generator(train_generator, epochs=epochs, verbose=1, callbacks=[tb_cb, lr_scheduler],\n",
    "                        validation_data=test_generator)\n",
    "\n",
    "    model.save(os.path.join(\"\", \"model_focalloss_0809.h5\"))\n",
    "\n",
    "    y_true = np.argmax(y_test, axis=-1)\n",
    "    y_pred = np.argmax(model.predict([x1_test, x2_test], batch_size=batch_size, verbose=1), axis=-1)\n",
    "\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
