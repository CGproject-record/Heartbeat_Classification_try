{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CGproject-record/Heartbeat_Classification_try/blob/main/20230808_pretrain_local_pc_try_load_bz2_file%20run%20colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a568207",
      "metadata": {
        "id": "8a568207"
      },
      "outputs": [],
      "source": [
        "# 20230803-pretrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499abcf1",
      "metadata": {
        "id": "499abcf1",
        "outputId": "e24e33ba-24a8-43af-8fe9-02da5a028152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 200, 1)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 64, 16)       192         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 64, 16)       64          conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 64, 16)       0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 31, 16)       0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 27, 32)       2592        max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 27, 32)       128         conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 27, 32)       0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 13, 32)       0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 11, 64)       6208        max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 11, 64)       256         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 11, 64)       0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 5, 64)        0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 320)          0           max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 4)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 324)          0           flatten_1[0][0]                  \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           20800       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 4)            260         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 30,500\n",
            "Trainable params: 30,276\n",
            "Non-trainable params: 224\n",
            "__________________________________________________________________________________________________\n",
            "training:\n",
            "50969/50969 [==============================] - 1s 15us/step\n",
            "[[45793    22     3     6]\n",
            " [   82   859     2     0]\n",
            " [   13     5  3764     6]\n",
            " [   32     0    16   366]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.997     0.999     0.998     45824\n",
            "           1      0.970     0.911     0.939       943\n",
            "           2      0.994     0.994     0.994      3788\n",
            "           3      0.968     0.884     0.924       414\n",
            "\n",
            "    accuracy                          0.996     50969\n",
            "   macro avg      0.982     0.947     0.964     50969\n",
            "weighted avg      0.996     0.996     0.996     50969\n",
            "\n",
            "testing:\n",
            "49661/49661 [==============================] - 1s 13us/step\n",
            "[[41420   671   206  1921]\n",
            " [  282  1494    57     3]\n",
            " [  141    21  3017    40]\n",
            " [  336     0    31    21]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.982     0.937     0.959     44218\n",
            "           1      0.683     0.814     0.743      1836\n",
            "           2      0.911     0.937     0.924      3219\n",
            "           3      0.011     0.054     0.018       388\n",
            "\n",
            "    accuracy                          0.925     49661\n",
            "   macro avg      0.647     0.685     0.661     49661\n",
            "weighted avg      0.959     0.925     0.941     49661\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import gzip\n",
        "import bz2\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n",
        "    y_true = K.cast(y_true, y_pred.dtype)\n",
        "\n",
        "    y_pred = K.one_hot(K.argmax(y_pred, axis=-1), num_classes=4)\n",
        "\n",
        "    tp = K.sum(y_true * y_pred, axis=0)\n",
        "    tn = K.sum((1 - y_true) * (1 - y_pred), axis=0)\n",
        "    fp = K.sum((1 - y_true) * y_pred, axis=0)\n",
        "    fn = K.sum(y_true * (1 - y_pred), axis=0)\n",
        "\n",
        "    precision = tp / (tp + fp + K.epsilon())\n",
        "    recall = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    return K.mean(2 * precision * recall / (precision + recall + K.epsilon()))\n",
        "\n",
        "\n",
        "def categorical_focal_loss(gamma=2):\n",
        "    \"\"\"\n",
        "        Categorical form of focal loss.\n",
        "            FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
        "        References:\n",
        "            https://arxiv.org/pdf/1708.02002.pdf\n",
        "        Usage:\n",
        "            model.compile(loss=categorical_focal_loss(gamma=2), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "            model.fit(class_weight={0:alpha0, 1:alpha1, ...}, ...)\n",
        "        Notes:\n",
        "           1. The alpha variable is the class_weight of keras.fit, so in implementation of the focal loss function\n",
        "           we needn't define this variable.\n",
        "           2. (important!!!) The output of the loss is the loss value of each training sample, not the total or average\n",
        "            loss of each batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n",
        "        y_true = K.cast(y_true, y_pred.dtype)\n",
        "\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "\n",
        "        return K.sum(-y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred), axis=-1)\n",
        "\n",
        "    return focal_loss\n",
        "\n",
        "# original\n",
        "# def load_data(filename=\"./dataset/mitdb.pkl\"):\n",
        "#     import pickle\n",
        "\n",
        "#     with open(filename, \"rb\") as f:\n",
        "#         (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = pickle.load(f)\n",
        "\n",
        "#     return (x1_train, x2_train, y_train), (x1_test, x2_test, y_test)\n",
        "\n",
        "# .gz\n",
        "# def load_data(filename=\"./dataset/mitdb.pkl.gz\"):\n",
        "#     import pickle\n",
        "\n",
        "#     with gzip.open(filename, \"rb\") as f:\n",
        "#         (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = pickle.load(f)\n",
        "\n",
        "#     return (x1_train, x2_train, y_train), (x1_test, x2_test, y_test)\n",
        "\n",
        "\n",
        "#  data = bz2.BZ2File(file, ‘rb’)\n",
        "# bz2\n",
        "def load_data(filename=\"./dataset/mitdb.pkl.bz2\"):\n",
        "    import pickle\n",
        "\n",
        "    with bz2.BZ2File(filename, \"rb\") as f:\n",
        "        (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = pickle.load(f)\n",
        "\n",
        "    return (x1_train, x2_train, y_train), (x1_test, x2_test, y_test)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = load_data()\n",
        "\n",
        "    x1_train = np.expand_dims(x1_train, axis=-1)\n",
        "    x1_test = np.expand_dims(x1_test, axis=-1)\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    x2_train = scaler.fit_transform(x2_train)\n",
        "    x2_test = scaler.transform(x2_test)\n",
        "\n",
        "    model = load_model(os.path.join(\"./models\", \"model_focalloss.h5\"),\n",
        "                       custom_objects={\"focal_loss\": categorical_focal_loss(gamma=2),\n",
        "                                       \"f1\": f1})\n",
        "    model.summary()\n",
        "\n",
        "    print(\"training:\")\n",
        "    y_true, y_pred = y_train, np.argmax(model.predict([x1_train, x2_train], batch_size=1024, verbose=1), axis=-1)\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred, digits=3))\n",
        "\n",
        "    print(\"testing:\")\n",
        "    y_true, y_pred = y_test, np.argmax(model.predict([x1_test, x2_test], batch_size=1024, verbose=1), axis=-1)\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f91424e5",
      "metadata": {
        "id": "f91424e5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}